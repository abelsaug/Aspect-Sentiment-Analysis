{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count Vectorized with aspect weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aspdep_weight(train_df, weight):\n",
    "    train_text = train_df[' text'].values.astype('U')\n",
    "    train_aspdep = train_df['asp_dep_words'].values.astype('U')\n",
    "    text_count_vect = CountVectorizer()\n",
    "    x_text_counts = text_count_vect.fit_transform(train_text)\n",
    "    text_voc = text_count_vect.vocabulary_\n",
    "    asp_dep_vect = CountVectorizer(vocabulary=text_voc)\n",
    "    x_aspdep_counts = asp_dep_vect.fit_transform(train_aspdep)\n",
    "    x_count_vec = x_text_counts + weight * x_aspdep_counts\n",
    "    x_tfidf_vec = TfidfTransformer(use_idf=True).fit_transform(x_count_vec)\n",
    "    return x_tfidf_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count Vectorized with aspect weight distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aspect_related_words(sdp, ardf):\n",
    "    print(\"Extracting aspect related words from text...\")\n",
    "    cols = list(ardf)\n",
    "    cols.append('asp_dep_words')\n",
    "    ar_df = pandas.DataFrame(columns=cols)\n",
    "    count = 0\n",
    "    for index, row in ardf.iterrows():\n",
    "        count += 1\n",
    "        print(count)\n",
    "        dep_set = set()\n",
    "        result = list(sdp.raw_parse(row[' text']))\n",
    "        parse_triples_list = [item for item in result[0].triples()]\n",
    "        for governor, dep, dependent in parse_triples_list:\n",
    "            print(\"G: \", governor, \"DEP: \", dep, \"depndent: \",dependent)\n",
    "            if governor[0] in row[' aspect_term'] or dependent[0] in row[' aspect_term']:\n",
    "                dep_set.add(governor[0])\n",
    "                dep_set.add(dependent[0])\n",
    "        ar_row = [row[c] for c in cols[:-1]]\n",
    "        ar_row.append(' '.join(list(dep_set)))\n",
    "        ar_df.loc[len(ar_df.index)] = ar_row\n",
    "    return ar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting aspect related words from text...\n",
      "1\n",
      "G:  ('features', 'VBZ') DEP:  advmod depndent:  ('Obviously', 'RB')\n",
      "G:  ('features', 'VBZ') DEP:  nsubj depndent:  ('one', 'PRP')\n",
      "G:  ('features', 'VBZ') DEP:  advmod depndent:  ('important', 'JJ')\n",
      "G:  ('features', 'VBZ') DEP:  dobj depndent:  ('interface', 'NN')\n",
      "G:  ('interface', 'NN') DEP:  compound depndent:  ('computer', 'NN')\n",
      "G:  ('interface', 'NN') DEP:  amod depndent:  ('human', 'JJ')\n",
      "2\n",
      "G:  ('browsing', 'VBD') DEP:  nsubj depndent:  ('web', 'NN')\n",
      "G:  ('web', 'NN') DEP:  amod depndent:  ('Good', 'JJ')\n",
      "G:  ('Good', 'JJ') DEP:  nmod:tmod depndent:  ('day', 'NN')\n",
      "G:  ('day', 'NN') DEP:  det depndent:  ('every', 'DT')\n",
      "G:  ('web', 'NN') DEP:  amod depndent:  ('computing', 'VBG')\n",
      "3\n",
      "G:  ('makes', 'VBZ') DEP:  nsubj depndent:  ('alright', 'NN')\n",
      "G:  ('alright', 'NN') DEP:  compound depndent:  ('keyboard', 'NN')\n",
      "G:  ('alright', 'NN') DEP:  appos depndent:  ('plate', 'NN')\n",
      "G:  ('plate', 'NN') DEP:  nmod depndent:  ('plastic', 'NN')\n",
      "G:  ('plastic', 'NN') DEP:  case depndent:  ('around', 'IN')\n",
      "G:  ('plastic', 'NN') DEP:  amod depndent:  ('cheap', 'JJ')\n",
      "G:  ('makes', 'VBZ') DEP:  dobj depndent:  ('sound', 'NN')\n",
      "G:  ('sound', 'NN') DEP:  amod depndent:  ('hollow', 'JJ')\n",
      "G:  ('sound', 'NN') DEP:  acl depndent:  ('using', 'VBG')\n",
      "G:  ('using', 'VBG') DEP:  dobj depndent:  ('buttons', 'NNS')\n",
      "G:  ('buttons', 'NNS') DEP:  compound depndent:  ('mouse', 'NN')\n",
      "G:  ('buttons', 'NNS') DEP:  compound depndent:  ('command', 'NN')\n",
      "4\n",
      "G:  ('work', 'VBP') DEP:  advmod depndent:  ('Again', 'RB')\n",
      "G:  ('work', 'VBP') DEP:  nsubj depndent:  ('speaker', 'NN')\n",
      "G:  ('speaker', 'NN') DEP:  compound depndent:  ('problem', 'NN')\n",
      "G:  ('speaker', 'NN') DEP:  amod depndent:  ('right', 'JJ')\n",
      "5\n",
      "G:  ('problem', 'NN') DEP:  nmod:poss depndent:  ('My', 'PRP$')\n",
      "G:  ('problem', 'NN') DEP:  dep depndent:  ('Service', 'NNP')\n",
      "G:  ('Service', 'NNP') DEP:  compound depndent:  ('DELL', 'NNP')\n",
      "G:  ('Service', 'NNP') DEP:  compound depndent:  ('Customer', 'NNP')\n",
      "                                                text            aspect_term  \\\n",
      "0  Obviously one important features computer huma...        human interface   \n",
      "1             Good every day computing web browsing.    every day computing   \n",
      "2  keyboard alright, plate around cheap plastic m...  mouse command buttons   \n",
      "3                Again, problem, right speaker work.          right speaker   \n",
      "4                  My problem DELL Customer Service.  DELL Customer Service   \n",
      "\n",
      "                       asp_dep_words  \n",
      "0  features human interface computer  \n",
      "1       computing day Good web every  \n",
      "2        using command mouse buttons  \n",
      "3         problem speaker right work  \n",
      "4      problem DELL Service Customer  \n"
     ]
    }
   ],
   "source": [
    "##TEST CELL\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "import pandas\n",
    "import nltk\n",
    "nltk.internals.config_java(\"C:\\Program Files\\Java\\jdk1.8.0_171\\\\bin\\java.exe\")\n",
    "sdp = StanfordDependencyParser(\n",
    "    path_to_jar=\"stanford-nlp-jars\\stanford-corenlp-full-2018-01-31\\stanford-corenlp-3.9.0.jar\",\n",
    "    path_to_models_jar=\"stanford-nlp-jars\\stanford-corenlp-full-2018-01-31\\stanford-corenlp-3.9.0-models.jar\")\n",
    "test_df = pandas.read_csv('test_1.csv', sep='\\t')\n",
    "print(extract_aspect_related_words(sdp, test_df[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-74bc272442ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     index=count_vect.get_feature_names()))\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mapply_aspdep_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-74bc272442ca>\u001b[0m in \u001b[0;36mapply_aspdep_weight\u001b[1;34m(train_df)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'U'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     train_aspdep = train_df['asp_dep_words'].values.astype('U')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtext_count_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mx_text_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_count_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     X_counts = count_vect.fit_transform(df1['skills_id'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "def apply_aspdep_weight(train_df):\n",
    "    train_text = train_df[' text'].values.astype('U')\n",
    "#     train_aspdep = train_df['asp_dep_words'].values.astype('U')\n",
    "    text_count_vect = CountVectorizer()\n",
    "    x_text_counts = text_count_vect.fit_transform(train_text)\n",
    "#     X_counts = count_vect.fit_transform(df1['skills_id'])\n",
    "    Xc = (x_text_counts.T * x_text_counts)\n",
    "    Xc.setdiag(0)\n",
    "    print(pd.DataFrame(Xc.todense(), \n",
    "    columns=count_vect.get_feature_names(), \n",
    "    index=count_vect.get_feature_names()))\n",
    "apply_aspdep_weight(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
